1) Arrancamos el docker:
P1:
sudo docker run --hostname=quickstart.cloudera --privileged=true -t -i --cpus=4 -v /home/lara/Escritorio/ICS/MapReduce/p1:/home/cloudera/practicas --publish-all=true -p 7180 cloudera/quickstart /usr/bin/docker-quickstart

P2:
sudo docker run --hostname=quickstart.cloudera --privileged=true -t -i --cpus=4 -v /home/lara/Escritorio/ICS/MapReduce/p2:/home/cloudera/practicas --publish-all=true -p 7180 cloudera/quickstart /usr/bin/docker-quickstart

P3:
sudo docker run --hostname=quickstart.cloudera --privileged=true -t -i --cpus=4 -v /home/lara/Escritorio/ICS/MapReduce/p3:/home/cloudera/practicas --publish-all=true -p 7180 cloudera/quickstart /usr/bin/docker-quickstart

2) Pasar una carpeta/fichero a formato hdfs

hdfs dfs -put /home/cloudera/practicas/files

3) ls de los ficheros que tenemos en el sistema de fich distribuido

hdfs dfs -ls /home/cloudera/practicas/files

4) Lanzar c√≥digo (con 2 reduces):
cd /home/cloudera/practicas

P1:
/usr/bin/hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.7.0.jar -D mapreduce.job.reduces=2 -input files -output salida -mapper mapper.py -reducer reducer.py -file mapper.py -file reducer.py

P2-P3:
/usr/bin/hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.7.0.jar -input files -output salida -mapper mapper.py -reducer reducer.py -file mapper.py -file reducer.py

5) Comprobar la salida:
hdfs dfs -ls

6) Recuperamos la salida:
hdfs dfs -get salida

7) Eliminamos los ficheros del hdfs:
hdfs dfs -rm -r files
hdfs dfs -rm -r salida

8) Procesar salida:
python3 filterresults.py salida/part-*
